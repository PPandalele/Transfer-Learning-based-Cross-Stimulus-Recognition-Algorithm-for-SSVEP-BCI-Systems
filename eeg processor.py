#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼ºç‰ˆTLCCAå®æ—¶è¯†åˆ«ç³»ç»Ÿ - æ”¯æŒæ¨¡å‹å’Œæ•°æ®é€‰æ‹©
"""

import numpy as np
import scipy.io as sio
import scipy.signal as signal
import time
import os

class TLCCAOnlineRecognition:
    # [EN] __init__: Auto-generated summary of this method's purpose.
    def __init__(self, subject_num=1, test_block=1, recognition_window=0.8, gui_mode=False):
        # åŸºç¡€å‚æ•°
        self.Fs = 250
        self.num_of_subbands = 5
        self.latency_delay = 0.0
        self.recognition_window = recognition_window
        self.subject_num = subject_num
        self.test_block = test_block
        
        self.gui_mode = False
        self.recognition_results = []  # å­˜å‚¨æ¯ä¸ªå­—ç¬¦çš„è¯†åˆ«ç»“æœ
        self.result_timestamps = [] 
        self.recognition_started = False
        # è‡ªåŠ¨æ„å»ºæ–‡ä»¶è·¯å¾„
        base_dir = r"D:\HuaweiMoveData\Users\PPand\Desktop\BCI_GUI"
        self.model_path = os.path.join(base_dir, "tlcca_models", f"S{subject_num}_tlcca_model_exclude_{test_block}.mat")
        self.test_data_path = os.path.join(base_dir, "test_data", f"S{subject_num}_blocks_{test_block}_test_data.mat")
        
    
        
        # å­—ç¬¦é›†
        self.beta_standard_chars = [
            '.', ',', '<', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 
            'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z',
            '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '_'
        ]
        

        self.num_of_harmonics = 5  # æ·»åŠ è¿™ä¸ªå±æ€§
        self.num_of_subbands = 5
        
        # ğŸ”‘ è®ºæ–‡å…¬å¼11çš„æ»¤æ³¢å™¨ç»„ç³»æ•°
        self.FB_coef = np.array([(n**(-1.25) + 0.25) for n in range(1, self.num_of_subbands + 1)])
        print(f"   FB coefficients: {self.FB_coef}")
        # æ¨¡å‹å‚æ•°
        self.online_weights = {}
        self.online_templates = {}
        self.target_order = None
        self.sti_f = None
        self.pha_val = None
        self.source_freq_idx = None
        self.target_freq_idx = None
        
        # å®æ—¶æ•°æ®
        self.source_data = None
        self.streaming_buffer = None
        self.received_samples = 0
        self.test_eeg_data = None

        self.subband_filters = {}
        for k in range(1, self.num_of_subbands + 1):
            Wp = np.array([(8*k)/(self.Fs/2), 90/(self.Fs/2)])
            Ws = np.array([(8*k-2)/(self.Fs/2), 100/(self.Fs/2)])
            
            try:
                N, Wn = signal.cheb1ord(Wp, Ws, 3, 40)
                b, a = signal.cheby1(N, 0.5, Wn, btype='band')
                self.subband_filters[k] = {'bpB': b, 'bpA': a}
            except:
                b, a = signal.butter(6, Wn, btype='band')
                self.subband_filters[k] = {'bpB': b, 'bpA': a}

        
        print("ğŸ§  TLCCA real-time recognition system initialized")

    # [EN] set_gui_mode: Auto-generated summary of this method's purpose.

    def set_gui_mode(self, recognition_queue=None):
        """è®¾ç½®GUIæ¨¡å¼"""
        self.gui_mode = True
        print("ğŸ“± Set to GUI mode")
    
    # [EN] run_gui_recognition: Auto-generated summary of this method's purpose.
    
    def run_gui_recognition(self):
        """GUIæ¨¡å¼ä¸‹è¿è¡Œè¯†åˆ« - æŒ‰ç…§åŸæœ¬çš„æ–¹å¼"""
        try:
            print("============================================================")
            print("ğŸš€ Start Beta simulation real-time recognition")
            print("============================================================")
            print(f"ğŸ”§ Configuration::")
            print(f"   Recognition window length: {self.recognition_window}s")
            
            self.recognition_started = True
            
            # æ¸…ç©ºç»“æœåˆ—è¡¨
            self.recognition_results = ['?'] * 40  # åˆå§‹åŒ–40ä¸ªå­—ç¬¦çš„ç»“æœ
            
            # æŒ‰ç…§åŸæœ¬çš„æ–¹å¼è¿è¡Œè¯†åˆ«
            self.run_real_time_recognition_for_gui()
            
        except Exception as e:
            print(f"âŒ GUI recognition run failed: {e}")
            
    # [EN] get_result_for_char: Auto-generated summary of this method's purpose.
            
    def get_result_for_char(self, char_index):
        """è·å–æŒ‡å®šå­—ç¬¦çš„è¯†åˆ«ç»“æœ"""
        if not self.recognition_started:
            return None  # è¯†åˆ«è¿˜æœªå¼€å§‹
            
        if 0 <= char_index < len(self.recognition_results):
            return self.recognition_results[char_index]
        return None
        
    # [EN] get_all_results: Auto-generated summary of this method's purpose.
        
    def get_all_results(self):
        """è·å–æ‰€æœ‰è¯†åˆ«ç»“æœ"""
        return self.recognition_results.copy()
    
    # [EN] get_result_for_char_with_delay: Auto-generated summary of this method's purpose.
    
    def get_result_for_char_with_delay(self, char_index, delay_seconds=2.0):
        """è·å–æŒ‡å®šå­—ç¬¦çš„è¯†åˆ«ç»“æœ - å¸¦å»¶è¿Ÿæ£€æŸ¥"""
        import time
        
        if not self.recognition_started:
            return None  # è¯†åˆ«è¿˜æœªå¼€å§‹
            
        if not (0 <= char_index < len(self.recognition_results)):
            return None
            
        # æ£€æŸ¥ç»“æœæ˜¯å¦å·²ç»äº§ç”Ÿ
        if self.result_timestamps[char_index] == 0:
            return None  # ç»“æœè¿˜æœªäº§ç”Ÿ
            
        # æ£€æŸ¥å»¶è¿Ÿæ—¶é—´æ˜¯å¦å·²è¿‡
        current_time = time.time()
        result_time = self.result_timestamps[char_index]
        
        if current_time - result_time >= delay_seconds:
            # å»¶è¿Ÿæ—¶é—´å·²è¿‡ï¼Œå¯ä»¥è¿”å›ç»“æœ
            return self.recognition_results[char_index]
        else:
            # è¿˜æ²¡åˆ°æ˜¾ç¤ºæ—¶é—´
            return None
        
    # [EN] run_real_time_recognition_for_gui: Auto-generated summary of this method's purpose.
        
    def run_real_time_recognition_for_gui(self):
        """ä¸ºGUIè¿è¡Œçš„å®æ—¶è¯†åˆ« - å®Œå…¨æŒ‰ç…§åŸæœ¬çš„æ–¹æ³•"""
        import time
        
        # è®¡ç®—å‚æ•°
        if self.subject_num <= 15:
            char_duration = 3.0
            flicker_duration = 2.0
        else:
            char_duration = 4.0
            flicker_duration = 3.0
        
        # æ¸…ç©ºç»“æœåˆ—è¡¨
        self.recognition_results = ['?'] * 40  # åˆå§‹åŒ–40ä¸ªå­—ç¬¦çš„ç»“æœ
        self.result_timestamps = [0] * 40     # åˆå§‹åŒ–æ—¶é—´æˆ³
        
        # è®°å½•è¯†åˆ«å¼€å§‹çš„çœŸå®æ—¶é—´
        recognition_start_time = time.time()
            
        # å¯¹æ¯ä¸ªå­—ç¬¦è¿›è¡Œè¯†åˆ« - æŒ‰ç…§åŸæœ¬çš„é€»è¾‘
        for char_idx in range(40):
            target_char = self.beta_standard_chars[char_idx]
            char_start_time = char_idx * char_duration
            
            # æŒ‰ç…§åŸæœ¬çš„æ—¶é—´çª—å£è®¡ç®—
            recognition_windows = []
            
            # è®¡ç®—è¯†åˆ«çª—å£ï¼ˆæŒ‰ç…§åŸæœ¬çš„é€»è¾‘ï¼‰
            window1_start = char_start_time + 0.63  # cue + ç”Ÿç†å»¶è¿Ÿ
            window1_end = window1_start + self.recognition_window
            
            # ç¡®ä¿ä¸è¶…å‡ºå­—ç¬¦æ—¶é—´èŒƒå›´
            if window1_end > char_start_time + char_duration:
                window1_end = char_start_time + char_duration
                
            if window1_end - window1_start >= 0.2:  # çª—å£è¶³å¤Ÿé•¿
                recognition_windows.append((window1_start, window1_end))
            
            best_result = '?'
            best_confidence = -1
            
            # å¯¹æ¯ä¸ªæ—¶é—´çª—å£è¿›è¡Œè¯†åˆ«
            for window_start, window_end in recognition_windows:
                start_sample = int(window_start * self.Fs)
                end_sample = int(window_end * self.Fs)
                
                if end_sample > self.total_samples:
                    end_sample = self.total_samples
                    
                if start_sample >= end_sample:
                    continue
                
                # æå–æ•°æ®çª—å£
                eeg_window = self.source_data[:, start_sample:end_sample]
                
                # è¯†åˆ«
                all_scores = self.calculate_tlcca_scores_for_all_chars(eeg_window)
                max_idx = np.argmax(all_scores)
                predicted_char = self.beta_standard_chars[max_idx]
                confidence = all_scores[max_idx]
                
                # æŒ‰ç…§åŸæœ¬çš„æ ¼å¼è¾“å‡º
                is_correct = "âœ…" if predicted_char == target_char else "âŒ"
                print(f"ğŸš€ Time {window_end:.2f}s | Samples{end_sample} | Char{char_idx}('{target_char}') | Window[{window_start:.2f}-{window_end:.2f}] | Relative{window_end - char_start_time:.3f}s | Window{window_end - window_start:.2f}s | Pred:'{predicted_char}' | {is_correct}")
                
                if confidence > best_confidence:
                    best_confidence = confidence
                    best_result = predicted_char
            
            # ä¿å­˜è¿™ä¸ªå­—ç¬¦çš„æœ€ç»ˆç»“æœå’Œæ—¶é—´æˆ³
            self.recognition_results[char_idx] = best_result
            self.result_timestamps[char_idx] = time.time()  # è®°å½•ç»“æœäº§ç”Ÿçš„çœŸå®æ—¶é—´
        
        print("============================================================")
        print("âœ… Beta simulation recognition finished")
        print("============================================================")

    # [EN] get_recognition_at_time: Auto-generated summary of this method's purpose.

    def get_recognition_at_time(self, char_index, current_time):
        """æ ¹æ®å­—ç¬¦ç´¢å¼•å’Œå½“å‰æ—¶é—´è·å–è¯†åˆ«ç»“æœ"""
        try:
            # è®¡ç®—å­—ç¬¦æ—¶é—´çª—å£
            if self.subject_num <= 15:
                char_duration = 3.0
            else:
                char_duration = 4.0
                
            char_start_time = char_index * char_duration
            
            # è®¡ç®—è¯†åˆ«çª—å£ï¼ˆä»cueç»“æŸ+ç”Ÿç†å»¶è¿Ÿå¼€å§‹ï¼‰
            recognition_start = char_start_time + 0.63  # 0.5s cue + 0.13s ç”Ÿç†å»¶è¿Ÿ
            
            # æ£€æŸ¥æ˜¯å¦åˆ°è¾¾è¯†åˆ«æ—¶é—´
            if current_time < recognition_start:
                return None  # è¿˜æ²¡åˆ°è¯†åˆ«æ—¶é—´
                
            # è®¡ç®—è¯†åˆ«çª—å£ç»“æŸæ—¶é—´
            window_end = min(recognition_start + self.recognition_window, char_start_time + char_duration)
            actual_time = min(current_time, window_end)
            
            # è®¡ç®—å®é™…ä½¿ç”¨çš„çª—å£é•¿åº¦
            window_length = actual_time - recognition_start
            if window_length < 0.2:  # çª—å£å¤ªçŸ­
                return None
                
            # æå–EEGæ•°æ®
            start_sample = int(recognition_start * self.Fs)
            end_sample = int(actual_time * self.Fs)
            
            if end_sample > self.total_samples:
                end_sample = self.total_samples
                
            if start_sample >= end_sample:
                return None
                
            eeg_window = self.source_data[:, start_sample:end_sample]
            
            # æ‰§è¡ŒtlCCAè¯†åˆ«
            all_scores = self.calculate_tlcca_scores_for_all_chars(eeg_window)
            
            # è¿”å›æœ€ä½³ç»“æœ
            max_idx = np.argmax(all_scores)
            predicted_char = self.beta_standard_chars[max_idx]
            confidence = all_scores[max_idx]
            
            result = {
                'char': predicted_char,
                'confidence': confidence,
                'window_length': window_length,
                'char_index': char_index
            }
            
            print(f"ğŸ§  Time {actual_time:.2f}s | Char{char_index}('{self.beta_standard_chars[char_index]}') | Window{window_length:.2f}s | Pred:'{predicted_char}' | ç½®ä¿¡åº¦:{confidence:.3f}")
            
            return result
            
        except Exception as e:
            print(f"âŒ Failed to get recognition result: {e}")
            return None

    # [EN] initialize_for_gui: Auto-generated summary of this method's purpose.

    def initialize_for_gui(self):
        """ä¸“ä¸ºGUIæ¨¡å¼åˆå§‹åŒ–"""
        success1 = self.load_pretrained_model(self.model_path)
        if not success1:
            return False
        
        success2 = self.load_source_data(self.test_data_path, self.test_block)
        if not success2:
            return False
        
        print("âœ… GUI mode initialization completed")
        return True

    # [EN] get_recognition_for_char: Auto-generated summary of this method's purpose.

    def get_recognition_for_char(self, char_idx, current_time):
        """ä¸ºGUIæä¾›ç‰¹å®šå­—ç¬¦çš„è¯†åˆ«ç»“æœ"""
        try:
            # æ ¹æ®å­—ç¬¦ç´¢å¼•å’Œå½“å‰æ—¶é—´ï¼Œæå–å¯¹åº”çš„EEGæ•°æ®çª—å£
            char_duration = 3.0 if self.subject_num <= 15 else 4.0
            char_start_time = char_idx * char_duration
            
            # è®¡ç®—è¯†åˆ«çª—å£
            recognition_start = char_start_time + 0.63  # cueæ—¶é—´ + ç”Ÿç†å»¶è¿Ÿ
            recognition_end = current_time
            window_duration = min(recognition_end - recognition_start, self.recognition_window)
            
            if window_duration < 0.2:  # çª—å£å¤ªå°
                return None
            
            # æå–EEGæ•°æ®çª—å£
            start_sample = int(recognition_start * self.Fs)
            end_sample = int(recognition_end * self.Fs)
            
            if end_sample > self.total_samples:
                return None
            
            eeg_window = self.source_data[:, start_sample:end_sample]
            
            # è¿›è¡Œè¯†åˆ«
            all_scores = self.calculate_tlcca_scores_for_all_chars(eeg_window)
            
            # è¿”å›æœ€é«˜åˆ†çš„å­—ç¬¦
            best_idx = np.argmax(all_scores)
            predicted_char = self.beta_standard_chars[best_idx]
            confidence = all_scores[best_idx]
            
            return {
                'char': predicted_char,
                'confidence': confidence,
                'scores': all_scores
            }
            
        except Exception as e:
            print(f"âŒ è¯†åˆ«Charæ—¶å‡ºé”™: {e}")
            return None
    

    # [EN] initialize_and_run: Auto-generated summary of this method's purpose.
    

    def initialize_and_run(self):
        """ä¸€é”®åˆå§‹åŒ–å’Œè¿è¡Œ"""
        if not self.load_pretrained_model(self.model_path):
            return False
        if not self.load_source_data(self.test_data_path, self.test_block):
            return False
        
        # è‡ªåŠ¨è®¡ç®—è¿è¡Œæ—¶é•¿
        if self.subject_num <= 15:
            char_duration = 3.0  # 0.5s + 2s + 0.5s
        else:
            char_duration = 4.0  # 0.5s + 3s + 0.5s
        
        total_duration = len(self.beta_standard_chars) * char_duration
        return self.run_real_time_recognition(total_duration)

    # [EN] select_model_and_data: Auto-generated summary of this method's purpose.

    def select_model_and_data(self):
        """é€‰æ‹©æ¨¡å‹å’Œæ•°æ®æ–‡ä»¶"""
        print("\n" + "="*50)
        print("ğŸ¯ TLCCA real-time recognition - model & data selection")
        print("="*50)
        
        # 1. é€‰æ‹©Subject
        print("\n1. Subject selection")
        print("-" * 30)
        while True:
            try:
                subject_num = int(input("è¯·è¾“å…¥Subjectç¼–å· (1-70): ").strip())
                if 1 <= subject_num <= 70:
                    break
                else:
                    print("âŒ Subject number must be between 1 and 70")
            except ValueError:
                print("âŒ Please enter a valid number")
        
        # 2. é€‰æ‹©æµ‹è¯•Block
        print(f"\n2. Test block selection")
        print("-" * 30)
        while True:
            try:
                test_block = int(input("è¯·è¾“å…¥æµ‹è¯•Blockç¼–å· (1-4): ").strip())
                if 1 <= test_block <= 4:
                    break
                else:
                    print("âŒ Block number must be between 1 and 4")
            except ValueError:
                print("âŒ Please enter a valid number")
        # ğŸ”‘ æ–°å¢ï¼šé€‰æ‹©çª—å£é•¿åº¦
        print("\nâ±ï¸ Recognition window lengthé€‰æ‹©:")
        print("1=0.3s, 2=0.4s, 3=0.5s, 4=0.6s, 5=0.7s")
        print("6=0.8s, 7=0.9s, 8=1.0s, 9=1.1s, 10=1.2s")

        while True:
            try:
                window_choice = input("è¯·é€‰æ‹©çª—å£é•¿åº¦ (1-10, é»˜è®¤6): ").strip()
                
                # çª—å£æ—¶é—´æ˜ å°„ï¼š0.3såˆ°1.2sï¼Œæ­¥é•¿0.1s
                window_options = [0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2]
                
                if window_choice == "":
                    window_duration = 0.8  # é»˜è®¤å€¼ï¼Œå¯¹åº”é€‰æ‹©6
                    break
                else:
                    choice_idx = int(window_choice) - 1
                    if 0 <= choice_idx < len(window_options):
                        window_duration = window_options[choice_idx]
                        break
                    else:
                        print("âŒ Please enter a number between 1 and 10")
            except ValueError:
                print("âŒ Please enter a valid number")

        print(f"âœ… é€‰æ‹©çš„Windowé•¿åº¦: {window_duration}s")
        
        # 3. æ„å»ºæ–‡ä»¶è·¯å¾„
        print(f"\n3. Build file paths")
        print("-" * 30)
        base_dir = r"D:\HuaweiMoveData\Users\PPand\Desktop\BCI_GUI"
        
        model_path = os.path.join(base_dir, "tlcca_models", f"S{subject_num}_tlcca_model_exclude_{test_block}.mat")
        test_data_path = os.path.join(base_dir, "test_data", f"S{subject_num}_blocks_{test_block}_test_data.mat")
        
        print(f"ğŸ“‚ Model path: {model_path}")
        print(f"ğŸ“‚ Test data path: {test_data_path}")
        
        # 4. æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        print(f"\n4. File existence check")
        print("-" * 30)
        if not os.path.exists(model_path):
            print(f"âŒ Model file does not exist: {model_path}")
            print("ğŸ’¡ Please run extract_block.py to generate the model file")
            return None, None, None, None
            
        if not os.path.exists(test_data_path):
            print(f"âŒ Test data file does not exist: {test_data_path}")
            print("ğŸ’¡ Please run extract_block.py to generate the test data file")
            return None, None, None, None
        
        print("âœ… All files exist")
        return model_path, test_data_path, subject_num, test_block

    # [EN] load_pretrained_model: Auto-generated summary of this method's purpose.

    def load_pretrained_model(self, model_path):
        """åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ - ä¿®å¤ç‰ˆ"""
        try:
            model_data = sio.loadmat(model_path)
            print(f"âœ… Model loaded: {model_path}")
            
            # ğŸ”‘ ä¿®å¤1ï¼šæŒ‰ç…§extract_block.pyçš„æ–¹å¼åŠ è½½æ˜ å°„ä¿¡æ¯
            if 'target_order' in model_data:
                self.target_order = model_data['target_order'].flatten()
                print(f"   Loaded target_order from model: {self.target_order[:5]}...")
            if 'sti_f' in model_data:
                self.sti_f = model_data['sti_f'].flatten()
                print(f"   Loaded frequencies from model: {self.sti_f[:5]}...")
            if 'pha_val' in model_data:
                self.pha_val = model_data['pha_val'].flatten()
                print(f"   Loaded phase info from model")
            if 'source_freq_idx' in model_data:
                self.source_freq_idx = model_data['source_freq_idx'].flatten()
                print(f"   Source domain indices: {self.source_freq_idx[:5]}...")
            if 'target_freq_idx' in model_data:
                self.target_freq_idx = model_data['target_freq_idx'].flatten()
                print(f"   Target domain indices: {self.target_freq_idx[:5]}...")
            
            # ğŸ”‘ ä¿®å¤2ï¼šå®Œå…¨æŒ‰ç…§try.pyçš„æˆåŠŸæ–¹å¼åŠ è½½æƒé‡
            self.online_weights = {}
            self.online_templates = {}
            
            weights_loaded = 0
            for sub_band in range(1, self.num_of_subbands + 1):
                # ğŸ”‘ å…³é”®ï¼šä½¿ç”¨ä¸extract_block.pyä¿å­˜æ—¶å®Œå…¨ç›¸åŒçš„é”®å
                wx_source_key = f'Wx_source_band{sub_band}'
                wy_source_key = f'Wy_source_band{sub_band}'
                wx_transfer_key = f'Wx_transfer_band{sub_band}'
                template_transfer_key = f'templates_transfer_band{sub_band}'
                
                # ğŸ”‘ ä¿®å¤ï¼šæŒ‰ç…§try.pyçš„æ–¹å¼ç›´æ¥ä¿å­˜åˆ°online_weights
                if wx_source_key in model_data:
                    self.online_weights[wx_source_key] = model_data[wx_source_key]
                    weights_loaded += 1
                if wy_source_key in model_data:
                    self.online_weights[wy_source_key] = model_data[wy_source_key]
                    weights_loaded += 1
                if wx_transfer_key in model_data:
                    self.online_weights[wx_transfer_key] = model_data[wx_transfer_key]
                    weights_loaded += 1
                if template_transfer_key in model_data:
                    self.online_templates[template_transfer_key] = model_data[template_transfer_key]
                    weights_loaded += 1
            
            print(f"âœ… Weights loaded: {weights_loaded}ä¸ª")
            
            # ğŸ”‘ ä¿®å¤3ï¼šéªŒè¯å­—ç¬¦-æƒé‡æ˜ å°„å…³ç³»
            print(f"\nğŸ” éªŒè¯Char-æƒé‡æ˜ å°„å…³ç³»:")
            for i in range(5):
                char = self.beta_standard_chars[i]
                reordered_pos = None
                for pos, orig_idx in enumerate(self.target_order):
                    if orig_idx == i:
                        reordered_pos = pos
                        break
                
                if reordered_pos is not None:
                    freq = self.sti_f[reordered_pos]
                    # æ£€æŸ¥ç¬¬ä¸€ä¸ªå­é¢‘å¸¦çš„æƒé‡
                    wx_source_key = f'Wx_source_band1'
                    wx_transfer_key = f'Wx_transfer_band1'
                    
                    source_weights = self.online_weights.get(wx_source_key, np.array([]))
                    transfer_weights = self.online_weights.get(wx_transfer_key, np.array([]))
                    
                    source_exists = (source_weights.size > 0 and i < source_weights.shape[1] and 
                                np.any(source_weights[:, i] != 0))
                    transfer_exists = (transfer_weights.size > 0 and i < transfer_weights.shape[1] and 
                                    np.any(transfer_weights[:, i] != 0))
                    
                    status = "âœ…" if (source_exists or transfer_exists) else "âŒ"
                    print(f"  Char'{char}'(orig idx{i}) -> reordered pos{reordered_pos} -> freq{freq:.1f}Hz -> weights exist{status}")
            
            return True
            
        except Exception as e:
            print(f"âŒ Model loading failed: {e}")
            return False

    # [EN] _init_default_mapping: Auto-generated summary of this method's purpose.

    def _init_default_mapping(self):
        """åˆå§‹åŒ–é»˜è®¤æ˜ å°„"""
        sti_f = np.array([8.6, 8.8, 9, 9.2, 9.4, 9.6, 9.8, 10, 10.2, 10.4, 10.6, 10.8,
                         11, 11.2, 11.4, 11.6, 11.8, 12, 12.2, 12.4, 12.6, 12.8,
                         13, 13.2, 13.4, 13.6, 13.8, 14, 14.2, 14.4, 14.6, 14.8,
                         15, 15.2, 15.4, 15.6, 15.8, 8, 8.2, 8.4])
        
        pha_val = np.array([0.0, 0.5, 1.0, 1.5] * 10)  # ä¸try.pyä¿æŒä¸€è‡´
        target_order = np.argsort(sti_f)  # æŒ‰é¢‘ç‡æ’åº
        sti_f = sti_f[target_order]  # é‡æ–°æ’åºé¢‘ç‡
        
        self.target_order = target_order
        self.sti_f = sti_f
        self.pha_val = pha_val
        self.source_freq_idx = np.arange(1, len(sti_f), 2)
        self.target_freq_idx = np.arange(0, len(sti_f), 2)

    # [EN] load_source_data: Auto-generated summary of this method's purpose.

    def load_source_data(self, test_data_path, test_block):
        """ä»MATæ–‡ä»¶åŠ è½½æµ‹è¯•æ•°æ® - ä¿®æ­£æ—¶é—´å¯¹åº”"""
        try:
            print(f"ğŸ“Š Load test data: {test_data_path}")
            test_data = sio.loadmat(test_data_path)
            
            data_key = f'block_{test_block}_data'
            if data_key not in test_data:
                print(f"âŒ æ‰¾ä¸åˆ°Data key: {data_key}")
                return False
            
            self.test_eeg_data = test_data[data_key]
            print(f"âœ… Test data loaded: {test_data_path}")
            print(f"   Data key: {data_key}")
            print(f"   Data shape: {self.test_eeg_data.shape}")
            
            if self.test_eeg_data.ndim == 3:
                channels, samples_per_trial, trials = self.test_eeg_data.shape
                
                # ğŸ”‘ å…³é”®ä¿®æ­£ï¼šä½¿ç”¨çœŸå®çš„2.0ç§’ä½œä¸ºå­—ç¬¦æ—¶é•¿
                # ğŸ”‘ æ ¹æ®è¢«è¯•ç¼–å·åŠ¨æ€è®¾ç½®å­—ç¬¦æ—¶é•¿
                if hasattr(self, 'subject_num') and self.subject_num <= 15:
                    self.char_duration = 3.0  # 1-15å·è¢«è¯•
                else:
                    self.char_duration = 4.0  # 16-70å·è¢«è¯•
                
                # ğŸ”‘ ä¿®æ­£ï¼šä½¿ç”¨çœŸå®çš„250Hzé‡‡æ ·ç‡
                actual_sampling_rate = 250  # ä¿®æ­£ä¸º250Hz
                print(f"   Actual duration per trial: {self.char_duration:.3f}s (åŸºäºMATLABåˆ†æ)")
                print(f"   æ¯ä¸ªtrialSamplesæ•°: {samples_per_trial}")
                print(f"   Sampling rate: {actual_sampling_rate}Hz")
                
                # é‡ç»„ä¸ºè¿ç»­æ•°æ®
                total_samples = samples_per_trial * trials
                self.source_data = np.zeros((channels, total_samples))
                
                for trial_idx in range(trials):
                    start_col = trial_idx * samples_per_trial
                    end_col = start_col + samples_per_trial
                    self.source_data[:, start_col:end_col] = self.test_eeg_data[:, :, trial_idx]
                
                # åˆå§‹åŒ–æµå¼ç¼“å†²åŒº
                self.streaming_buffer = np.zeros((channels, 0))
                self.received_samples = 0
                
                # ğŸ”‘ ä¿®å¤ï¼šæ·»åŠ ç¼ºå¤±çš„å±æ€§
                self.total_samples = total_samples  # æ€»æ ·æœ¬æ•°
                self.channels = channels  # é€šé“æ•°
                
                print(f"âœ… Source data prepared: {self.source_data.shape}")
                print(f"   Corrected total duration: {self.source_data.shape[1]/self.Fs:.2f}ç§’")
                print(f"   ä¿®æ­£åCharæ—¶é•¿: {self.char_duration:.3f}s")
                print(f"   Charæ•°é‡: {trials}")
                print(f"   æ€»Samplesæ•°: {self.total_samples}")  # ğŸ”‘ æ·»åŠ è°ƒè¯•ä¿¡æ¯
            
                
                # ğŸ”‘ æ˜¾ç¤ºæ­£ç¡®çš„å­—ç¬¦æ—¶é—´çª—å£
                print(f"   CharTimeWindow:")
                for i in range(min(5, trials)):
                    start_time = i * self.char_duration
                    end_time = (i + 1) * self.char_duration
                    analysis_time = start_time + self.latency_delay
                    char = self.beta_standard_chars[i] if i < len(self.beta_standard_chars) else '?'
                    print(f"     Char{i}('{char}'): {start_time:.3f}-{end_time:.3f}s, analysis from{analysis_time:.3f}så¼€å§‹")
                    
                return True
            else:
                print(f"âŒ Data dimension error: {self.test_eeg_data.shape}")
                return False
                
        except Exception as e:
            print(f"âŒ Data loading failed: {e}")
            return False

    # [EN] simulate_data_streaming: Auto-generated summary of this method's purpose.

    def simulate_data_streaming(self, current_time):
        """çœŸæ­£çš„250Hzå®æ—¶æ•°æ®æµ - æ¯4msä¸€ä¸ªæ ·æœ¬"""
        # ğŸ”‘ å…³é”®ï¼šæŒ‰250Hzä¸¥æ ¼è®¡ç®—åº”è¯¥æœ‰å¤šå°‘æ ·æœ¬
        expected_samples = int(current_time * self.Fs)
        
        # ğŸ”‘ é€ä¸ªæ ·æœ¬åœ°æ·»åŠ ï¼Œæ¨¡æ‹ŸçœŸå®çš„è¿ç»­é‡‡é›†
        while self.received_samples < expected_samples and self.received_samples < self.total_samples:
            # è·å–ä¸‹ä¸€ä¸ªæ ·æœ¬
            new_sample = self.source_data[:, self.received_samples:self.received_samples+1]
            
            # æ·»åŠ åˆ°æµç¼“å†²åŒºï¼ˆçœŸæ­£çš„è¿ç»­è¿½åŠ ï¼‰
            if self.streaming_buffer.shape[1] == 0:
                self.streaming_buffer = new_sample
            else:
                # ğŸ”‘ è¿ç»­è¿½åŠ ï¼Œä¸é‡æ–°åˆ†é…å†…å­˜
                if self.streaming_buffer.shape[1] <= self.received_samples:
                    # æ‰©å±•ç¼“å†²åŒº
                    extension = np.zeros((self.streaming_buffer.shape[0], 1000))
                    self.streaming_buffer = np.concatenate([self.streaming_buffer, extension], axis=1)
                
                self.streaming_buffer[:, self.received_samples] = new_sample.flatten()
            
            self.received_samples += 1
        
        return self.received_samples

    # [EN] get_data_window: Auto-generated summary of this method's purpose.

    def get_data_window(self, window_start, window_end):
        """è·å–æ•°æ®çª—å£"""
        start_sample = int(window_start * self.Fs)
        end_sample = int(window_end * self.Fs)
        available_samples = self.streaming_buffer.shape[1]
        
        start_sample = max(0, start_sample)
        end_sample = min(end_sample, available_samples)
        
        if start_sample >= end_sample:
            return None
        
        return self.streaming_buffer[:, start_sample:end_sample]

    # [EN] apply_subband_filter: Auto-generated summary of this method's purpose.

    def apply_subband_filter(self, data, sub_band):
        """å­é¢‘å¸¦æ»¤æ³¢ - ä½¿ç”¨ä¸è®­ç»ƒæ—¶ç›¸åŒçš„æ»¤æ³¢å™¨"""
        try:
            if data.shape[1] < 10:
                return data
            
            # ä½¿ç”¨é¢„å…ˆè®¾è®¡å¥½çš„æ»¤æ³¢å™¨
            if sub_band in self.subband_filters:
                b = self.subband_filters[sub_band]['bpB']
                a = self.subband_filters[sub_band]['bpA']
                filtered_data = signal.filtfilt(b, a, data, axis=1)
                return filtered_data
            else:
                return data
        except:
            return data

    # [EN] generate_reference_signals: Auto-generated summary of this method's purpose.

    def generate_reference_signals(self, freq, phase, length):
        """ç”Ÿæˆå‚è€ƒä¿¡å· - è®ºæ–‡æ ‡å‡†å¤šè°æ³¢"""
        try:
            t = np.arange(length) / self.Fs
            ref_signals = []
            
            # ğŸ”‘ ç”Ÿæˆå¤šä¸ªè°æ³¢çš„æ­£å¼¦å’Œä½™å¼¦ä¿¡å·
            for harmonic in range(1, self.num_of_harmonics + 1):
                harmonic_freq = freq * harmonic
                cos_signal = np.cos(2 * np.pi * harmonic_freq * t + phase * harmonic)
                sin_signal = np.sin(2 * np.pi * harmonic_freq * t + phase * harmonic)
                ref_signals.extend([cos_signal, sin_signal])
            
            return np.array(ref_signals)  # (2*harmonics, length)
        except:
            return np.zeros((2*self.num_of_harmonics, length))

    # [EN] calculate_correlation: Auto-generated summary of this method's purpose.

    def calculate_correlation(self, x, y):
        """è®¡ç®—ç›¸å…³ç³»æ•°"""
        try:
            if len(x) != len(y) or len(x) < 2:
                return 0.0
            
            x = x - np.mean(x)
            y = y - np.mean(y)
            
            x_std = np.std(x)
            y_std = np.std(y)
            
            if x_std == 0 or y_std == 0:
                return 0.0
            
            correlation = np.corrcoef(x, y)[0, 1]
            if np.isnan(correlation):
                return 0.0
            
            return abs(correlation)
        except:
            return 0.0


    # [EN] matlab_canoncorr_exact: Auto-generated summary of this method's purpose.


    def matlab_canoncorr_exact(self, X, Y):
        """MATLAB canoncorrå®ç° - ä¸tlcca beta.pyå®Œå…¨ä¸€è‡´"""
        if X.ndim == 1:
            X = X.reshape(-1, 1)
        if Y.ndim == 1:
            Y = Y.reshape(-1, 1)
        
        # ä¸­å¿ƒåŒ–
        X = X - np.mean(X, axis=0)
        Y = Y - np.mean(Y, axis=0)
        
        n = X.shape[0]
        
        # è®¡ç®—åæ–¹å·®çŸ©é˜µï¼ˆä½¿ç”¨MATLABçš„æ–¹å¼ï¼‰
        Cxx = (X.T @ X) / (n - 1)
        Cyy = (Y.T @ Y) / (n - 1) 
        Cxy = (X.T @ Y) / (n - 1)
        
        try:
            # ä½¿ç”¨Choleskyåˆ†è§£ï¼Œæ›´æ¥è¿‘MATLAB
            Lx = np.linalg.cholesky(Cxx + 1e-12 * np.eye(Cxx.shape[0]))
            Ly = np.linalg.cholesky(Cyy + 1e-12 * np.eye(Cyy.shape[0]))
            
            # æ±‚è§£å¹¿ä¹‰ç‰¹å¾å€¼é—®é¢˜
            M = np.linalg.solve(Lx, Cxy)
            M = np.linalg.solve(Ly.T, M.T).T
            
            U, s, Vt = np.linalg.svd(M, full_matrices=False)
            
            A = np.linalg.solve(Lx.T, U[:, 0])
            B = np.linalg.solve(Ly.T, Vt[0, :])
            
            return A, B
            
        except:
            # å¤‡ç”¨æ–¹æ¡ˆ
            from sklearn.cross_decomposition import CCA
            cca = CCA(n_components=1)
            cca.fit(X, Y)
            return cca.x_weights_[:, 0], cca.y_weights_[:, 0]
    
    # [EN] run_real_time_recognition: Auto-generated summary of this method's purpose.
    
    def run_real_time_recognition(self, duration=80.0):
        """è¿è¡Œå®æ—¶è¯†åˆ« - çœŸæ­£æ¨¡æ‹Ÿå®æ—¶æ•°æ®æµ"""
        print("\n" + "="*60)
        print("ğŸš€ å¼€å§‹çœŸå®Timeå®æ—¶è¯†åˆ«")
        print("="*60)
        print(f"ğŸ”§ Configuration::")
        print(f"   Recognition window length: {self.recognition_window}s")
        
        recognition_count = 0
        correct_count = 0
        trial_results = {}
        
        # ğŸ”‘ å…³é”®ï¼šä½¿ç”¨çœŸå®æ—¶é—´ï¼Œè€Œä¸æ˜¯ç¨‹åºå¾ªç¯æ—¶é—´
        start_real_time = time.time()
        update_interval = 0.02  # 20msæ›´æ–°é—´éš”
        window_duration = self.recognition_window  # ğŸ”‘ ä½¿ç”¨å¯¹è±¡çš„çª—å£å‚æ•°
        if hasattr(self, 'subject_num') and self.subject_num <= 15:
            char_duration = 3.0  # 1-15å·è¢«è¯•: 0.5s+2s+0.5s = 3s
        else:
            char_duration = 4.0  # 16-70å·è¢«è¯•: 0.5s+3s+0.5s = 4s
        
        while True:
            # ğŸ”‘ å…³é”®ä¿®å¤ï¼šè·å–çœŸå®æµé€çš„æ—¶é—´
            current_time = time.time() - start_real_time
            
            if current_time >= duration:
                break
            
            # ğŸ”‘ å…³é”®ï¼šæ¨¡æ‹ŸçœŸå®æ•°æ®æµ - æ•°æ®è¿ç»­ä¸æ–­åœ°ä»¥250Hzé‡‡é›†
            self.simulate_data_streaming(current_time)
            
            # ğŸ”‘ ç¡®å®šå½“å‰å±äºå“ªä¸ªå­—ç¬¦
            current_char_idx = int(current_time / char_duration)
            
            if current_char_idx >= len(self.beta_standard_chars):
                break
            
            # ğŸ”‘ è®¡ç®—å½“å‰å­—ç¬¦çš„æ—¶é—´èŒƒå›´
            char_start_time = current_char_idx * char_duration
            char_end_time = char_start_time + char_duration
            
   
            # è®¡ç®—å­—ç¬¦å†…çš„ç›¸å¯¹æ—¶é—´
            char_relative_time = current_time - char_start_time
            cue_duration = 0.5  # æç¤ºæ—¶é—´
            physiological_delay = 0.13  # ç”Ÿç†å»¶è¿Ÿ
            recognition_start_offset = cue_duration + physiological_delay  # 0.63ç§’

            # ğŸ”‘ ä¿®æ”¹ï¼šæ ¹æ®SubjectåŠ¨æ€è®¾ç½®è¯†åˆ«ç»“æŸæ—¶é—´
            if hasattr(self, 'subject_num') and self.subject_num <= 15:
                flicker_duration = 2.0  # 1-15å·è¢«è¯•ï¼š2ç§’é—ªçƒ
            else:
                flicker_duration = 3.0  # 16-70å·è¢«è¯•ï¼š3ç§’é—ªçƒ

            recognition_end_offset = cue_duration + flicker_duration  # åŠ¨æ€è®¡ç®—ç»“æŸæ—¶é—´

            # åªåœ¨æœ‰æ•ˆè¯†åˆ«çª—å£å†…è¿›è¡Œè¯†åˆ«
            if (char_start_time <= current_time <= char_end_time and 
                recognition_start_offset <= char_relative_time <= recognition_end_offset):
                
          
                # çª—å£ç»“æŸæ—¶é—´æ˜¯å½“å‰æ—¶é—´
                window_end_time = current_time
                window_start_time = window_end_time - window_duration

                # ğŸ”‘ ç¡®ä¿çª—å£ä¸æ—©äºè¯†åˆ«å¼€å§‹æ—¶é—´
                recognition_absolute_start = char_start_time + recognition_start_offset
                if window_start_time < recognition_absolute_start:
                    window_start_time = recognition_absolute_start
                
                # ğŸ”‘ è½¬æ¢ä¸ºæ ·æœ¬ç´¢å¼•
                window_start_sample = int(window_start_time * self.Fs)
                window_end_sample = int(window_end_time * self.Fs)
                
                # ğŸ”‘ æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„æ•°æ®ï¼ˆè‡³å°‘0.5ç§’çš„æ•°æ®ï¼‰
                # ğŸ”‘ æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„æ•°æ®ï¼ˆæ ¹æ®çª—å£å¤§å°åŠ¨æ€è°ƒæ•´ï¼‰
                min_samples = int(max(0.2, window_duration * 0.8) * self.Fs)  # è‡³å°‘çª—å£çš„80%æˆ–0.2s  # è‡³å°‘125ä¸ªæ ·æœ¬
                expected_samples = window_end_sample - window_start_sample
                
                if (window_end_sample <= self.received_samples and 
                    expected_samples >= min_samples):
                    
                    # æå–çª—å£æ•°æ®
                    data_window = self.streaming_buffer[:, window_start_sample:window_end_sample]
                    
                   
                    if data_window.shape[1] >= min_samples:
                        
                    
                        all_scores = self.calculate_tlcca_scores_for_all_chars(data_window)
                        
                        # é€‰æ‹©æœ€é«˜åˆ†æ•°çš„å­—ç¬¦
                        best_idx = np.argmax(all_scores)
                        predicted_char = self.beta_standard_chars[best_idx]
                        target_char = self.beta_standard_chars[current_char_idx]
                        # è®¡ç®—å­—ç¬¦ç›¸å¯¹æ—¶é—´
                        char_relative_time = current_time - char_start_time
                        actual_window_duration = data_window.shape[1] / self.Fs
                        
                        is_correct = predicted_char.lower() == target_char.lower()
                        
                        # è®°å½•ç»“æœ
                        recognition_count += 1
                        if current_char_idx not in trial_results:
                            trial_results[current_char_idx] = {
                                'target': target_char, 
                                'predictions': [], 
                                'correct': 0, 
                                'total': 0
                            }
                        
                        trial_results[current_char_idx]['predictions'].append(predicted_char)
                        trial_results[current_char_idx]['total'] += 1
                        
                        if is_correct:
                            correct_count += 1
                            trial_results[current_char_idx]['correct'] += 1
                        
                        # æ˜¾ç¤ºè¯†åˆ«çŠ¶æ€
                        status = "âœ…" if is_correct else "âŒ"
                        
                        print(f"ğŸš€ Time{current_time:5.2f}s | Samples{self.received_samples} | "
                            f"Char{current_char_idx}('{target_char}') | Window[{window_start_time:.2f}-{window_end_time:.2f}] | "
                            f"Relative{char_relative_time:.3f}s | Window{actual_window_duration:.2f}s | Pred:'{predicted_char}' | {status}")
            
            # ğŸ”‘ å…³é”®ï¼šä¸¥æ ¼æŒ‰20msé—´éš”æ›´æ–°
            time.sleep(update_interval)

        
        # æœ€ç»ˆç»Ÿè®¡
        print("\n" + "="*60)
        print("ğŸ“Š Recognition statistics")
        print("="*60)
        
        for trial_idx, stats in trial_results.items():
            final_pred = max(set(stats['predictions']), key=stats['predictions'].count)
            acc = stats['correct'] / stats['total'] * 100
            final_correct = "âœ…" if final_pred == stats['target'] else "âŒ"
            print(f"Trial {trial_idx:2d}: '{stats['target']}' -> '{final_pred}' ({acc:5.1f}%) {final_correct}")
        
        overall_acc = (correct_count / recognition_count * 100) if recognition_count > 0 else 0
        trial_acc = sum(1 for s in trial_results.values() 
                    if max(set(s['predictions']), key=s['predictions'].count) == s['target']) / len(trial_results) * 100
        
        print(f"\nğŸ¯ Final results:")
        print(f"   Per-trial accuracy: {trial_acc:.1f}%")
        print(f"   Real-time accuracy: {overall_acc:.1f}%")
        print(f"   Recognition count: {recognition_count}")
        print(f"   å¹³å‡æ¯CharRecognition count: {recognition_count/len(trial_results):.1f}")
        
        return trial_results



    # [EN] calculate_tlcca_scores_for_all_chars: Auto-generated summary of this method's purpose.



    def calculate_tlcca_scores_for_all_chars(self, eeg_window):
        """è®¡ç®—æ‰€æœ‰å­—ç¬¦çš„TLCCAåˆ†æ•° - ä¸tlcca beta.pyé€»è¾‘ä¸€è‡´"""
        
        # æ•°æ®é¢„å¤„ç† - 50Hzæ»¤æ³¢
        try:
            processed_window = np.zeros_like(eeg_window)
            
            # æ„å»ºä¸è®­ç»ƒæ—¶ç›¸åŒçš„å¤šæ¬¡è°æ³¢é™·æ³¢æ»¤æ³¢å™¨
            Fo = 50
            Q = 35
            M = int(np.floor((self.Fs/2) / Fo))  # M=2
            notchB = np.array([1.0])
            notchA = np.array([1.0])
            
            for k in range(1, M+1):
                w0 = (k * Fo) / (self.Fs/2)
                b_k, a_k = signal.iirnotch(w0, Q)
                notchB = np.convolve(notchB, b_k)
                notchA = np.convolve(notchA, a_k)
            
            for ch in range(eeg_window.shape[0]):
                processed_window[ch, :] = signal.filtfilt(notchB, notchA, eeg_window[ch, :])
            eeg_window = processed_window
        except:
            pass
        
        # å¯¹æ‰€æœ‰å­—ç¬¦è®¡ç®—åˆ†æ•°
        all_scores = []
        
        for char_idx in range(len(self.beta_standard_chars)):
            # æ‰¾åˆ°å­—ç¬¦çš„é‡æ’åºä½ç½®
            reordered_pos = None
            for pos, orig_idx in enumerate(self.target_order):
                if orig_idx == char_idx:
                    reordered_pos = pos
                    break
            
            if reordered_pos is None:
                all_scores.append(0.0)
                continue
            
            # è·å–é¢‘ç‡å’Œç›¸ä½
            freq = self.sti_f[reordered_pos]
            phase = self.pha_val[reordered_pos]
            Y1 = self.generate_reference_signals(freq, phase, eeg_window.shape[1])
            
            total_score = 0.0
            
            # å¯¹æ¯ä¸ªå­é¢‘å¸¦è®¡ç®—
            for sub_band in range(1, self.num_of_subbands + 1):
                X_filtered = self.apply_subband_filter(eeg_window, sub_band)
                
                # è®¡ç®—å•ä¸ªå­—ç¬¦åœ¨å½“å‰å­é¢‘å¸¦çš„åˆ†æ•°
                rho_i = self.calculate_single_char_score(X_filtered, Y1, reordered_pos, sub_band)
                
                # è®ºæ–‡å…¬å¼11ï¼šåŠ æƒæ±‚å’Œ
                fb_weight = self.FB_coef[sub_band-1]
                total_score += fb_weight * rho_i
            
            all_scores.append(total_score)
        
        return all_scores

    # [EN] calculate_single_char_score: Auto-generated summary of this method's purpose.

    def calculate_single_char_score(self, X_filtered, Y1, reordered_pos, sub_band):
        """è®¡ç®—å•ä¸ªå­—ç¬¦åœ¨å•ä¸ªå­é¢‘å¸¦çš„åˆ†æ•°"""
        
        wx_source_key = f'Wx_source_band{sub_band}'
        wy_source_key = f'Wy_source_band{sub_band}'  
        wx_transfer_key = f'Wx_transfer_band{sub_band}'
        templates_transfer_key = f'templates_transfer_band{sub_band}'

        r1a = r1b = r3 = 0.0

        # æ£€æŸ¥æ˜¯å¦ä¸ºæºåŸŸå­—ç¬¦
        source_domain_idx = None
        for i, freq_idx in enumerate(self.source_freq_idx):
            if freq_idx == reordered_pos:
                source_domain_idx = i
                break

        target_domain_idx = None
        for i, freq_idx in enumerate(self.target_freq_idx):
            if freq_idx == reordered_pos:
                target_domain_idx = i
                break

        # è®ºæ–‡å…¬å¼10ï¼šä¸‰ä¸ªåˆ†é‡
        if reordered_pos in self.source_freq_idx and source_domain_idx is not None:
            # æºåŸŸå­—ç¬¦ï¼šä½¿ç”¨æºåŸŸæƒé‡
            if wx_source_key in self.online_weights and wy_source_key in self.online_weights:
                try:
                    W1_x = self.online_weights[wx_source_key][:, source_domain_idx]
                    W1_y = self.online_weights[wy_source_key][:, source_domain_idx] 
                    
                    if np.any(W1_x != 0) and np.any(W1_y != 0):
                        X_proj = W1_x.T @ X_filtered
                        Y_proj = W1_y.T @ Y1
                        r1a = self.calculate_correlation(X_proj.flatten(), Y_proj.flatten())
                except Exception as e:
                    r1a = 0.0
            
            r1b = 0.0  # æºåŸŸå­—ç¬¦æ²¡æœ‰ç¬¬äºŒåˆ†é‡
            
        elif reordered_pos in self.target_freq_idx and target_domain_idx is not None:
            r1a = 0.0  # ç›®æ ‡åŸŸå­—ç¬¦æ²¡æœ‰ç¬¬ä¸€åˆ†é‡
            
            # ç›®æ ‡åŸŸå­—ç¬¦ï¼šä½¿ç”¨ä¼ é€’å­¦ä¹ æƒé‡
            corresponding_source_idx = target_domain_idx
            
            if (wx_transfer_key in self.online_weights and 
                templates_transfer_key in self.online_templates):
                try:
                    W2_x = self.online_weights[wx_transfer_key][:, corresponding_source_idx]
                    H1_r1 = self.online_templates[templates_transfer_key][:, corresponding_source_idx]
                    
                    if np.any(W2_x != 0) and np.any(H1_r1 != 0):
                        X_proj = W2_x.T @ X_filtered
                        template_len = min(len(H1_r1), len(X_proj))
                        if template_len > 10:
                            r1b = self.calculate_correlation(
                                X_proj[:template_len].flatten(), 
                                H1_r1[:template_len].flatten()
                            )
                except Exception as e:
                    r1b = 0.0

        # ç¬¬ä¸‰åˆ†é‡ï¼šCCAè®¡ç®—
        if reordered_pos in self.source_freq_idx and source_domain_idx is not None:
            if wx_source_key in self.online_weights:
                try:
                    W1_x = self.online_weights[wx_source_key][:, source_domain_idx]
                    if np.any(W1_x != 0):
                        try:
                            # å®Œæ•´CCAè®¡ç®—
                            filtered_test_signal = W1_x.T @ X_filtered
                            filtered_test_reshaped = filtered_test_signal.reshape(-1, 1)
                            ref1_reshaped = Y1.T
                            
                            A1, B1 = self.matlab_canoncorr_exact(filtered_test_reshaped, ref1_reshaped)
                            proj1 = filtered_test_reshaped @ A1.reshape(-1, 1)
                            proj2 = ref1_reshaped @ B1.reshape(-1, 1)
                            r3 = self.calculate_correlation(proj1.flatten(), proj2.flatten())
                        except:
                            # å¤‡ç”¨æ–¹æ¡ˆï¼šç®€å•ç›¸å…³æ€§
                            min_len = min(len(filtered_test_signal), Y1.shape[1])
                            r3 = self.calculate_correlation(
                                filtered_test_signal[:min_len], 
                                Y1[0, :min_len]
                            )
                except Exception as e:
                    r3 = 0.0
                    
        elif reordered_pos in self.target_freq_idx and target_domain_idx is not None:
            corresponding_source_idx = target_domain_idx
            if wx_transfer_key in self.online_weights:
                try:
                    W1_x = self.online_weights[wx_transfer_key][:, corresponding_source_idx]
                    if np.any(W1_x != 0):
                        try:
                            # å®Œæ•´CCAè®¡ç®—
                            filtered_test_signal = W1_x.T @ X_filtered
                            filtered_test_reshaped = filtered_test_signal.reshape(-1, 1)
                            ref1_reshaped = Y1.T
                            
                            A1, B1 = self.matlab_canoncorr_exact(filtered_test_reshaped, ref1_reshaped)
                            proj1 = filtered_test_reshaped @ A1.reshape(-1, 1)
                            proj2 = ref1_reshaped @ B1.reshape(-1, 1)
                            r3 = self.calculate_correlation(proj1.flatten(), proj2.flatten())
                        except:
                            # å¤‡ç”¨æ–¹æ¡ˆï¼šç®€å•ç›¸å…³æ€§
                            min_len = min(len(filtered_test_signal), Y1.shape[1])
                            r3 = self.calculate_correlation(
                                filtered_test_signal[:min_len], 
                                Y1[0, :min_len]
                            )
                except Exception as e:
                    r3 = 0.0

        # è®ºæ–‡å…¬å¼10ï¼šÏi = Î£(sign(ri,m) * ri,m^2)
        rho_i = (np.sign(r1a) * r1a**2 + np.sign(r1b) * r1b**2 + np.sign(r3) * r3**2)
        
        return rho_i
    # [EN] precise_sleep_until: Auto-generated summary of this method's purpose.
    def precise_sleep_until(self, target_time):
        """é«˜ç²¾åº¦ç¡çœ ç›´åˆ°ç›®æ ‡æ—¶é—´"""
        while True:
            current = time.time()
            remaining = target_time - current
            if remaining <= 0:
                break
            if remaining > 0.001:
                time.sleep(remaining - 0.001)
            else:
                time.sleep(0.0001)  # 100å¾®ç§’ç²¾åº¦

# [EN] main: Auto-generated summary of this method's purpose.

def main():
    """ç®€åŒ–çš„ä¸»å‡½æ•° - æ”¯æŒGUIè°ƒç”¨"""
    # è¿™ä¸ªå‡½æ•°ç°åœ¨ä¸»è¦ç”¨äºç‹¬ç«‹æµ‹è¯•
    # GUIè°ƒç”¨æ—¶ä¼šç›´æ¥å®ä¾‹åŒ–ç±»å¹¶è°ƒç”¨ç›¸åº”æ–¹æ³•
    pass

# ä¸ºGUIæä¾›çš„ä¾¿æ·åˆ›å»ºå‡½æ•°
# [EN] create_for_gui: Auto-generated summary of this method's purpose.
def create_for_gui(subject_num, test_block, recognition_window):
    """ä¸ºGUIåˆ›å»ºè¯†åˆ«å™¨å®ä¾‹"""
    recognizer = TLCCAOnlineRecognition(
        subject_num=subject_num,
        test_block=test_block,
        recognition_window=recognition_window,
        gui_mode=True
    )
    return recognizer


if __name__ == "__main__":
    main()